# -*- coding: utf-8 -*-
"""cyberbullying-tweet-recognition-project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pz2LkMtcYJFJs37_sSMV8XmtP92HZUX9

# Context
With rise of social media coupled with the Covid-19 pandemic, cyberbullying has reached all time highs. We can combat this by creating models to automatically flag potentially harmful tweets as well as break down the patterns of hatred.

# About Dataset
As social media usage becomes increasingly prevalent in every age group, a vast majority of citizens rely on this essential medium for day-to-day communication. Social mediaâ€™s ubiquity means that cyberbullying can effectively impact anyone at any time or anywhere, and the relative anonymity
of the internet makes such personal attacks more difficult to stop than traditional bullying.

On April 15th, 2020, UNICEF issued a warning in response to the increased risk of cyberbullying during the COVID-19 pandemic due to widespread school closures, increased screen time, and decreased face-to-face social interaction. The statistics of cyberbullying are outright alarming: 36.5% of middle and high school students have felt cyberbullied and 87% have observed cyberbullying, with effects ranging from decreased academic performance to depression to suicidal thoughts.

# Variables
In light of all of this, this dataset contains more than 47000 tweets labelled according to the class of cyberbullying:

__Age
Ethnicity
Gender
Religion
Other type of cyberbullying
Not cyberbullying__

# Objective
The objective of this project is to create a model that can automatically flag potentially harmful tweets as well as break down the patterns of hatred

# Importing Libraries
"""

!pip install emoji
!pip install pyspark

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import plotly.express as px
import emoji
import string
import nltk
from PIL import Image
from collections import Counter
from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC,LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
import pickle

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive')

# membuat sesi baru
from pyspark.sql import SparkSession, SQLContext
spark = SparkSession.builder.appName('UAS BIG DATA').getOrCreate()

"""# Getting data"""

data = spark.read.csv("/content/drive/MyDrive/Colab Notebooks/BIG DATA/cyberbullying_tweets.csv", inferSchema=True, header=True)

"""# Initial Review"""

data.show(10)

# melakukan filter csv sesuai dengan kategori cyberbullying

filtered_dataset = data.na.drop(subset=["cyberbullying_type", "tweet_text"])\
              .select(['tweet_text', 'cyberbullying_type']).dropDuplicates()\
              .filter(
                  (data.cyberbullying_type == "religion") |
                  (data.cyberbullying_type == "age") |
                  (data.cyberbullying_type == "gender") |
                  (data.cyberbullying_type == "ethnicity") |
                  (data.cyberbullying_type == "not_cyberbullying") |
                  (data.cyberbullying_type == "other_cyberbullying")
              )

filtered_dataset.show(10)

# memasukkan file csv ke dalam temporary table / view dengan nama tweets
filtered_dataset.createOrReplaceTempView("tweets")

# print schema dan juga isi dari tabel tweets
sql = spark.sql("SELECT * FROM tweets")

sql.printSchema()
sql.show(5)

# konversi dari tabel sql spark ke dataframe pandas
data = sql.toPandas()

# menampilkan info data dari tabel tweets
data.info()

data.head()

"""# Checking Missing Values"""

data.isnull().sum()

"""## No Missing Values found!"""

data['cyberbullying_type'].value_counts()

"""## Renaming columns for better understanding"""

data = data.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})

data.head()

"""### Adding Encoded column for sentiments"""

data["sentiment_encoded"] = data['sentiment'].replace({"religion": 1, "age": 2, "ethnicity": 3, "gender": 4, "other_cyberbullying": 5,"not_cyberbullying": 6})

data.head()

stop_words = set(stopwords.words('english'))

"""# Preprocessing of Text

## Function to Remove Emojis
"""

def strip_emoji(text):
    return emoji.replace_emoji(text,replace="")

"""## Function to Convert text to lowercase, remove (/r, /n  characters), URLs, non-utf characters, Numbers, punctuations,stopwords"""

def strip_all_entities(text):
    text = text.replace('\r', '').replace('\n', ' ').lower()
    text = re.sub(r"(?:\@|https?\://)\S+", "", text)
    text = re.sub(r'[^\x00-\x7f]',r'', text)
    text = re.sub(r'(.)1+', r'1', text)
    text = re.sub('[0-9]+', '', text)
    stopchars= string.punctuation
    table = str.maketrans('', '', stopchars)
    text = text.translate(table)
    text = [word for word in text.split() if word not in stop_words]
    text = ' '.join(text)
    return text

"""## Function to remove contractions"""

def decontract(text):
    text = re.sub(r"can\'t", "can not", text)
    text = re.sub(r"n\'t", " not", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'s", " is", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'t", " not", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'m", " am", text)
    return text

"""## Function to Clean Hashtags"""

def clean_hashtags(tweet):
    new_tweet = " ".join(word.strip() for word in re.split('#(?!(?:hashtag)\b)[\w-]+(?=(?:\s+#[\w-]+)*\s*$)', tweet))
    new_tweet2 = " ".join(word.strip() for word in re.split('#|_', new_tweet))
    return new_tweet2

"""## Function to Filter Special Characters such as $, &"""

def filter_chars(a):
    sent = []
    for word in a.split(' '):
        if ('$' in word) | ('&' in word):
            sent.append('')
        else:
            sent.append(word)
    return ' '.join(sent)

"""## Function to remove mutiple sequence spaces"""

def remove_mult_spaces(text):
    return re.sub("\s\s+" , " ", text)

"""## Function to apply stemming to words"""

def stemmer(text):
    tokenized = nltk.word_tokenize(text)
    ps = PorterStemmer()
    return ' '.join([ps.stem(words) for words in tokenized])

"""## Function to apply lemmatization to words"""

def lemmatize(text):
    tokenized = nltk.word_tokenize(text)
    lm = WordNetLemmatizer()
    return ' '.join([lm.lemmatize(words) for words in tokenized])

"""## Function to Preprocess the text by applying all above functions"""

def preprocess(text):
    text = strip_emoji(text)
    text = decontract(text)
    text = strip_all_entities(text)
    text = clean_hashtags(text)
    text = filter_chars(text)
    text = remove_mult_spaces(text)
    text = stemmer(text)
    text = lemmatize(text)
    return text

data['cleaned_text'] = data['text'].apply(preprocess)
data.head()

"""## Cleaned text added

## Dealing with Duplicates
"""

data["cleaned_text"].duplicated().sum()

data.drop_duplicates("cleaned_text", inplace=True)

"""# Duplicates removed

# Tokenization
"""

data['tweet_list'] = data['cleaned_text'].apply(word_tokenize)
data.head()

"""# Checking length of various tweet texts"""

text_len = []
for text in data.tweet_list:
    tweet_len = len(text)
    text_len.append(tweet_len)
data['text_len'] = text_len

plt.figure(figsize=(15,8))
ax = sns.countplot(x='text_len', data=data, palette='mako')
plt.title('Count of words in tweets', fontsize=20)
plt.yticks([])
ax.bar_label(ax.containers[0])
plt.ylabel('count')
plt.xlabel('')
plt.show()

"""# Removing text without words"""

data = data[data['text_len']!=0]

data.shape

"""# Function to create WordCloud"""

def plot_wordcloud(cyberbullying_type):
    string = ""
    for i in data[data.sentiment == cyberbullying_type].cleaned_text.values:
        string = string + " " + i.strip()

    custom_mask = np.array(Image.open('/content/drive/MyDrive/Colab Notebooks/BIG DATA/twitter_mask.png'))
    mask_colors = ImageColorGenerator(custom_mask)
    wordcloud = WordCloud(background_color ='white',max_words=2000, max_font_size=256,
               random_state=42, width=custom_mask.shape[1],height=custom_mask.shape[0],
                mask = custom_mask,min_font_size = 10).generate(string)

    # plot the WordCloud image
    plt.figure(figsize = (8, 8), facecolor = None)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.tight_layout(pad = 0)
    plt.title(cyberbullying_type)
    plt.show()
    del string

"""# Splitting data based on sentiment for Data Exploration"""

not_cyberbullying_type = data[data['sentiment']=='not_cyberbullying']
gender_type = data[data['sentiment']=='gender']
religion_type = data[data['sentiment']=='religion']
other_cyberbullying_type = data[data['sentiment']=='other_cyberbullying']
age_type = data[data['sentiment']=='age']
ethnicity_type = data[data['sentiment']=='ethnicity']

"""# Exploratory Data Analysis

# Gender Based Cyberbullying
"""

gender = Counter([item for sublist in gender_type['tweet_list'] for item in sublist])
top20_gender = pd.DataFrame(gender.most_common(20))
top20_gender.columns = ['Top Words','Count']
top20_gender.style.background_gradient(cmap='Greens')

fig = plt.figure(figsize=(15,8))
sns.barplot(data=top20_gender, y="Count", x="Top Words")
plt.title("Top 20 words in Gender Cyberbullying")

plot_wordcloud('gender')

"""# Religion Based Cyberbullying"""

religion = Counter([item for sublist in religion_type['tweet_list'] for item in sublist])
top20_religion = pd.DataFrame(religion.most_common(20))
top20_religion.columns = ['Top Words','Count']
top20_religion.style.background_gradient(cmap='Greens')

fig = plt.figure(figsize=(15,8))
sns.barplot(data=top20_religion, y="Count", x="Top Words")
plt.title("Top 20 words in Religion Cyberbullying")

plot_wordcloud('religion')

"""# Age based Cyberbullying"""

age = Counter([item for sublist in age_type['tweet_list'] for item in sublist])
top20_age = pd.DataFrame(age.most_common(20))
top20_age.columns = ['Top Words','Count']
top20_age.style.background_gradient(cmap='Greens')

fig = plt.figure(figsize=(15,8))
sns.barplot(data=top20_age, y="Count", x="Top Words")
plt.title("Top 20 words in Age Cyberbullying")

plot_wordcloud('age')

"""# Ethnicity based Cyberbullying"""

ethnicity = Counter([item for sublist in ethnicity_type['tweet_list'] for item in sublist])
top20_ethnicity = pd.DataFrame(ethnicity.most_common(20))
top20_ethnicity.columns = ['Top Words','Count']
top20_ethnicity.style.background_gradient(cmap='Greens')

fig = plt.figure(figsize=(15,8))
sns.barplot(data=top20_ethnicity, y="Count", x="Top Words")
plt.title("Top 20 words in Ethnicity Cyberbullying")

plot_wordcloud('ethnicity')

"""# Other types of Cyberbullying"""

other_cyberbullying = Counter([item for sublist in other_cyberbullying_type['tweet_list'] for item in sublist])
top20_other_cyberbullying = pd.DataFrame(other_cyberbullying.most_common(20))
top20_other_cyberbullying.columns = ['Top Words','Count']
top20_other_cyberbullying.style.background_gradient(cmap='Greens')

fig = plt.figure(figsize=(15,8))
sns.barplot(data=top20_other_cyberbullying, y="Count", x="Top Words")
plt.title("Top 20 words in Other Cyberbullying")

plot_wordcloud('other_cyberbullying')

"""# Tweets without Cyberbullying"""

not_cyberbullying = Counter([item for sublist in not_cyberbullying_type['tweet_list'] for item in sublist])
top20_not_cyberbullying = pd.DataFrame(not_cyberbullying.most_common(20))
top20_not_cyberbullying.columns = ['Top Words','Count']
top20_not_cyberbullying.style.background_gradient(cmap='Greens')

fig = plt.figure(figsize=(15,8))
sns.barplot(data=top20_not_cyberbullying, y="Count", x="Top Words")
plt.title("Top 20 words in Not Cyberbullying")

plot_wordcloud('not_cyberbullying')

data.head()

sentiments = ["religion", "age", "ethnicity", "gender", "other_cyberbullying","not_cyberbullying"]

"""# Splitting Data into Train and Test Sets"""

X,Y = data['cleaned_text'],data['sentiment_encoded']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, stratify =Y, random_state = 42)
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

"""## tf-idf Vectorization"""

tf_idf = TfidfVectorizer()
X_train_tf = tf_idf.fit_transform(X_train)
X_test_tf = tf_idf.transform(X_test)
print(X_train_tf.shape)
print(X_test_tf.shape)

"""# Trying Different ML Models

## Logistic Regression
"""

log_reg = LogisticRegression()

log_cv_score = cross_val_score(log_reg,X_train_tf,y_train,cv=5,scoring='f1_macro',n_jobs=-1)

mean_log_cv = np.mean(log_cv_score)
mean_log_cv

"""## Support Vector Classifier"""

lin_svc = LinearSVC()

lin_svc_cv_score = cross_val_score(lin_svc,X_train_tf,y_train,cv=5,scoring='f1_macro',n_jobs=-1)
mean_lin_svc_cv = np.mean(lin_svc_cv_score)
mean_lin_svc_cv

"""## Naive Bayes Classifier"""

multiNB = MultinomialNB()

multiNB_cv_score = cross_val_score(multiNB,X_train_tf,y_train,cv=5,scoring='f1_macro',n_jobs=-1)
mean_multiNB_cv = np.mean(multiNB_cv_score)
mean_multiNB_cv

"""## Decison Tree Classifier"""

dtree = DecisionTreeClassifier()

dtree_cv_score = cross_val_score(dtree,X_train_tf,y_train,cv=5,scoring='f1_macro',n_jobs=-1)
mean_dtree_cv = np.mean(dtree_cv_score)
mean_dtree_cv

"""## By trying different models we can see logistic regression, svm and random forest classifier performed similarly, so among these we will go with svm model as it is more generalised and light

## Fine Tuning Support Vector Classifier
"""

svc1 = LinearSVC()
param_grid = {'C':[0.0001,0.001,0.01,0.1,1,10],
             'loss':['hinge','squared_hinge'],
             'fit_intercept':[True,False]}
grid_search = GridSearchCV(svc1,param_grid,cv=5,scoring='f1_macro',n_jobs=-1,verbose=0,return_train_score=True)
grid_search.fit(X_train_tf,y_train)

grid_search.best_estimator_

grid_search.best_score_

"""# Model Evaluation"""

lin_svc.fit(X_train_tf,y_train)
y_pred = lin_svc.predict(X_test_tf)

def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):
    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)
    fig = plt.figure(figsize=figsize)
    try:
        heatmap = sns.heatmap(df_cm, annot=True, fmt="d")
    except ValueError:
        raise ValueError("Confusion matrix values must be integers.")
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)
    plt.ylabel('Truth')
    plt.xlabel('Prediction')

cm = confusion_matrix(y_test,y_pred)
print_confusion_matrix(cm,sentiments)

print('Classification Report:\n',classification_report(y_test, y_pred, target_names=sentiments))

